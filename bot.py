# This Python script sets up a chatbot that uses the GPT-4 model to generate responses based on semantic similarity to a set of documents.
# The chatbot is accessible via a web interface.
# The index.json file stores a vector representation for each document in the dataset, which is a sort of 'summary' of the document that has been transformed into a mathematical format (vector) that can be compared and analyzed.
# When a user inputs a question or prompt, the GPT-4 model is used in two ways:
# 1. To convert the input prompt into the same vector representation as the documents. This way, the input prompt can be compared to the stored documents in the index.json.
# 2. To generate a response based on the most similar document(s) to the input vector.
# Example:  "Here's what you asked me (the prompt), and from all the documents I know about (the index), this document (or these documents) are the most similar or relevant. So, here's a response (generated by GPT-4) based on that information."
# The index.json file serves as a 'searchable index' for your documents. When a user enters a prompt, the system does not send the entire index.json content to GPT-4. Instead, it uses the index to identify the document(s) most semantically similar to the user's prompt.

from gpt_index import (
    SimpleDirectoryReader,
    GPTListIndex,
    GPTSimpleVectorIndex,
    LLMPredictor,
    PromptHelper,
)
from langchain.chat_models import ChatOpenAI
import gradio as gr
import os

os.environ["OPENAI_API_KEY"] = "sk-**";


# This function builds an index using the GPT-4 model.
# It reads documents from a directory, and for each document, generates vector representations using the model.
# These vectors are then stored in an index that can be used to answer queries based on their semantic similarity to the input text.
def construct_index(directory_path):
    max_input_size = 4096
    num_outputs = 512
    max_chunk_overlap = 20
    chunk_size_limit = 600

    #  It assists in generating the prompt for the model. It also helps in chunking the input text into smaller pieces.
    prompt_helper = PromptHelper(
        max_input_size,
        num_outputs,
        max_chunk_overlap,
        chunk_size_limit=chunk_size_limit,
    )

    # Is an instance of the language model predictor which uses the GPT-4 model: will be used to generate vector representations for the documents.
    llm_predictor = LLMPredictor(
        llm=ChatOpenAI(
            temperature=0.7, model_name="gpt-4", max_tokens=num_outputs
        )
    )

    # It reads the documents from the directory and returns a list of Document objects.
    documents = SimpleDirectoryReader(directory_path).load_data()

    # Generates the index.json from the documents using the GPT-4 model. It saves the index to disk.
    index = GPTSimpleVectorIndex(
        documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper
    )

    index.save_to_disk("index.json")

    return index

# This function loads the previously saved index.json and uses it to generate a response to a given input text. It returns the chatbot's response.
def chatbot(input_text):
    index = GPTSimpleVectorIndex.load_from_disk("index.json")
    response = index.query(input_text, response_mode="compact")
    return response.response

# sets up a web interface for the chatbot. It uses the chatbot function to process input and generate responses.
# The Gradio interface provides a textbox for users to type their queries and displays the chatbot's responses.
iface = gr.Interface(
    fn=chatbot,
    inputs=gr.components.Textbox(lines=7, label="Enter your text"),
    outputs="text",
    title="Custom-trained AI Chatbot",
)

# The script creates the index and launches the Gradio interface.
# The share=True argument means that a publicly shareable link to the interface is created (server runs locally)
index = construct_index("docs")
iface.launch(share=False,  debug=True)
